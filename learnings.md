## New stack learnings

- The TS worker drops scraped products without a non-empty `itemId` before calling `products:ingestRun`; we now persist this as `runs.missingItemIds` (set on completion) so the UI can warn per source even though those products never reach `productsLatest`.
- `links:countsBySource` now returns `missingItemIds` from the latest successful run, alongside linked/unlinked counts.
- The multi-source unlinked queue uses `links:listUnlinkedPage` which merges per-source scans in-memory; it reports `truncated: true` when it had to cap per-source scanning.
- `Panel` needed broader typing (`ComponentPropsWithoutRef<"div">`) to allow click handlers like `onClick` in TS.
- Link workbench bulk actions are backed by Convex `links:bulkLink` (deduped up to 250 items) plus `links:getUnlinkedByKeys` for validating bulk selections.
- Link suggestions are currently heuristic (token overlap + name substring) via `links:suggestCanonicalsForProduct` (per item) and `links:smartSuggestions` (bulk groups); if suggestion quality becomes an issue, tighten thresholds or add domain-specific signals (brand/vendor tokens, SKU normalization).
- For “manual” sources (e.g. Amazon before a scraper exists), writing directly to `productsLatest` + `pricePoints` without setting `sources.lastSuccessfulRunId` avoids “latest run” filtering hiding previously-entered items; `products:listLatest`/`links:*` fall back to `by_sourceSlug_lastSeenAt` when there is no `lastSuccessfulRunId`.
- Amazon opportunity logic now lives in Convex (`amazon:pricingOpportunities`): it joins `canonicalProducts` + `productLinks` + `productsLatest` and classifies `undercut`/`raise`/`watch` plus missing-data buckets; the `/amazon-pricing` page consumes it directly.
- Scraper monitoring UI now lives at `/scrapers` (overview) and `/scrapers/history` + `/scrapers/history/:runId` (run history + detail); `/history` is currently an alias to the same history page.
- `runs:listRecent` now allows up to 200 rows, which keeps the history page usable without adding a dedicated filtered index yet.
- Scraper Builder runs use queue payload `configOverride` + `dryRun` so the worker can test arbitrary configs without ingesting into `productsLatest`; these runs are created with `requestedBy: "builder"` so `runs:setStatus` intentionally does not update `sources.lastSuccessfulRunId/At`.
- Source management now lives at `/scrapers/sources` with create/edit pages and a “Test (dry-run)” action backed by Convex `sourcesActions:startDryRun`; these runs are created with `requestedBy: "test"` and `runs:setStatus` skips updating `sources.lastSuccessfulRunId/At` for them.
- Scraper Builder now supports multiple saved drafts via Convex tables `scraperBuilderDrafts` + `scraperBuilderState` (keyed by `ownerKey` like `user:default`), and the builder dry-run action takes a `draftId` so logs/results stay attached to the selected draft.
- Shopify collection tag pages (`/collections/<handle>/<tag>`) often need `constraint=<tag>` on the `products.json` URL; we now support `config.constraint`, and the builder auto-detect sets it from the URL path/query.
- After changing Convex schema/functions locally, run `cd new && ./apps/web/node_modules/.bin/convex dev --once --typecheck disable --codegen disable` to push updates to the self-hosted backend (Docker restart alone won’t deploy them).
- “Automation pause” is implemented via Redis key `corastuff:automation:paused` (toggled through enqueuer endpoints `/schedules/status|pause|resume`); the worker skips scheduled jobs before creating a run, so manual “Run now” stays unaffected.
- Scraper concurrency is persisted in Convex `settings` (`key: "scraperConcurrencyLimit"`) and applied to BullMQ worker `concurrency` on startup (requires worker restart to take effect).
- Shopify vendor listing scraping now uses `cheerio` to extract product handles from the vendor HTML (more robust than regex, handles `/en/products/<handle>`-style links).
- Amazon storefront scraping is implemented as `scrapeAmazonStorefront` in `new/packages/scrapers/src/amazonStorefront.ts`; the worker auto-detects it when `storeUrl`/`sourceUrl` is an `amazon.*` URL (and has a fallback config for `amazon_de`).
- When you add/change exports in `@corastuff/scrapers`, rebuild it (`pnpm -C new --filter @corastuff/scrapers build`) before building `@corastuff/worker`, since the worker consumes the built `dist` types/exports.
- When adding new config-driven scrapers in the worker, make the `parse*Config` helpers validate the target hostname (e.g. `bergzeit.de` vs `globetrotter.de`) so generic keys like `listingUrl` don’t accidentally route a source to the wrong scraper.
- Bike24 is Cloudflare-protected; fetching the listing via `https://r.jina.ai/<listingUrl>` returns stable Markdown lines with product links, `images.bike24.com` image URLs, and `€` prices (good enough for discovery without Playwright).
- `r.jina.ai` can be slow/spiky (tens of seconds); if runs become flaky, add an explicit fetch timeout (AbortController) and/or retries around the Jina request.
- Bunert PDP scraping works well via Playwright + lightweight stealth init script (hide `navigator.webdriver`, set `navigator.languages`/`navigator.plugins`); the product metadata is available via `window.dataLayer.ecommerce.detail.products` and the hero image via `meta[property="og:image"]`.
- Decathlon CH brand listing scraping is implemented as `scrapeDecathlonChBrandPage` in `new/packages/scrapers/src/decathlonCh.ts` and auto-detected by the worker when the config `listingUrl` host is `decathlon.ch`.
- Decathlon CH price strings often use `.` as a decimal separator; when parsing, don’t blindly strip dots as thousand separators or you’ll turn `33.50` into `3350`.
